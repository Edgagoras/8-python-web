{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Notebook 8.2 HTML-soup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Not all data on-line will be conventiently accessible through a REST API, and in those cases we'll need to get and parse the HTML text data directly.  This is much more difficult and tedious than using the rest API, since we'll need to look at the HTML representation of the webpage directly and try to understand it in order to find the part of the code that we're interested in. There are several Python packages for parsing HTML and trying to make sense of it, and for some terrible reason the best of them has the worst name, and is called `beautifulsoup4`. Install this package with conda and import it to continue. \n",
    "\n",
    "Then read through the \"Quick Start\" guide for Beautiful soup [here]([Documentation](https://beautiful-soup-4.readthedocs.io/en/latest/).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# conda install beautifulsoup4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### What is HTML?\n",
    "\n",
    "HTML is the language of the web. It stands for hyper text markup language. In fact, the `markdown` language that we have been using to write text in our notebooks is simply an abstraction of HTML (markup language), which is easier to write but gets rendered into HTML after we execute it. The two languages are identical under the hood and can be used interchangeably in many places. \n",
    "\n",
    "To get a firm grasp on how HTML works, you can complete the tutorial on https://www.w3schools.com/html/html_intro.asp. This is entirely necessary for us to proceed, but it's useful reading if you're interested. The main thing to know, however, is that HTML is a hierarchical structure (things nested within things) and that each of those things is labelled by a type of tag. For example, in the link above you can see that a webpage has a `<HTML>` tag, and `<head>` tag, and a `<body>` tag. These are simply nested containers for writing text into. The entire web is simply a bunch of text, with design instructions laid on top of it. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Let's take a look at some HTML\n",
    "Open the URL below in either chrome or firefox. In either of these browsers right-click on any link in the page and from the dropdown menu select \"inspect\". This should open a new window that shows the HTML element that corresponds to the tag you are inspecting. Look at the name of the tag. For example, if you clicked on a student's email address you would see `<div class=\"student-email\">` above that element. This is a specific element for storing email addresses. Now that we know that, if we wanted to get all of the email addresses for students in the department we know that we could parse the HTML of the webpage and try to just pull out all of the 'student-email' elements. So let's give that a try by using beautifulsoup. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "baseurl = \"http://e3b.columbia.edu/students/current/phd/\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### get the HTML using requests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'<!DOCTYPE html>\\n<html lang=\"en-US\" prefix=\"og: http://ogp.me/ns# fb: http://ogp.me/ns/fb#\">\\n<head>\\n    <meta charset=\"UTF-8\" />\\n    <title>Current - Ph.D. - Columbia University Ecology, Evolution and Environmental Biology Department</title>\\n\\t<link rel=\"shortcut icon\" type=\"image/x-icon\" href=\"http://e3b.columbia.edu/wp-content/themes/columbia-e3b/assets/ico/favicon.ico?v=2\">\\n\\t<link rel=\"icon\" type=\"image/ico\" href=\"http://e3b.columbia.edu/wp-content/themes/columbia-e3b/assets/ico/favicon.ico>v=2'"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# get the full page HTML using requests, print hte first 500 characters\n",
    "response = requests.get(baseurl)\n",
    "response.text[:500]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### parse the HTML using bs4\n",
    "[Documentation](https://beautiful-soup-4.readthedocs.io/en/latest/)  \n",
    "\n",
    "Here we can use the `.find()` or `find_all()` functions to search for particular tags (anything within a \"< >\") as well as class or id attributes of those tags. Here is another pretty good guide for further tips on using bs4 (https://www.dataquest.io/blog/web-scraping-tutorial-python/). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a BS object from the URL\n",
    "soup = BeautifulSoup(response.text, \"html5lib\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "# find all div elements with class='student-email'\n",
    "emails = soup.find_all(\"div\", {\"class\", \"student-email\"})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tb2583@columbia.edu\n",
      "yc2975@columbia.edu\n",
      "pc2796@columbia.edu \n",
      "bdc2120@columbia.edu\n",
      "lac2208@columbia.edu\n",
      "hf2306@columbia.edu\n",
      "jsh2211@columbia.edu\n",
      "sah2216@columbia.edu\n",
      "amh2284@columbia.edu\n",
      "jej2141@columbia.edu\n",
      "pak2136@columbia.edu\n",
      "sk4335@columbia.edu\n",
      "sk4220@columbia,edu\n",
      "pfm2119@columbia.edu\n",
      "an2601@columbia.edu\n",
      "arp2195@columbia.edu\n",
      "awq2101@columbia.edu\n",
      "vr2352@columbia.edu\n",
      "prp2123@columbia.edu\n",
      "lr2767@columbia.edu\n",
      "scs2204@columbia.edu\n",
      "sss2254@columbia.edu\n",
      "ss4812@columbia,edu\n",
      "mqt2101@columbia.edu\n",
      "bnt2111 @columbia.edu\n",
      "jc4055@columbia.edu\n",
      "mv2640@columbia.edu\n",
      "nkw2113@columbia.edu\n"
     ]
    }
   ],
   "source": [
    "for email in emails:\n",
    "    atag = email.find('a')\n",
    "    if atag:\n",
    "        print(atag.contents[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### A little more advanced\n",
    "You can see when inspecting the HTML that right above the `student-email` tags there are also tags for `student-name` and `student-program`. Let's collect all of this data into dictionary. The other two elements are even simpler to parse than emails, since they return plain text instead of a link (`<a>` tag). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "# find all div elements with class='student-email'\n",
    "emails = soup.find_all(\"div\", {\"class\", \"student-email\"})\n",
    "names = soup.find_all(\"div\", {\"class\", \"student-name\"})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [],
   "source": [
    "students = {}\n",
    "\n",
    "for name, email in zip(names, emails):\n",
    "    \n",
    "    name = name.contents[0].strip()\n",
    "    atag = email.find(\"a\")\n",
    "    if name and atag:\n",
    "        students[name] = atag.contents[0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'Bytnerowicz, Thomas': 'tb2583@columbia.edu',\n",
       " 'Cheng, Yi-Ru': 'yc2975@columbia.edu',\n",
       " 'Choksi, Pooja': 'pc2796@columbia.edu ',\n",
       " 'Clark, Benjamin': 'bdc2120@columbia.edu',\n",
       " 'Coelho, Lais': 'lac2208@columbia.edu',\n",
       " 'Fuong, Holly': 'hf2306@columbia.edu',\n",
       " 'Hall, Jazlynn': 'jsh2211@columbia.edu',\n",
       " 'Heilpern, Sebastian': 'sah2216@columbia.edu',\n",
       " 'Huddell, Alex': 'amh2284@columbia.edu',\n",
       " 'Jensen, Johanna (Jo)': 'jej2141@columbia.edu',\n",
       " 'Kache, Pallavi': 'pak2136@columbia.edu',\n",
       " 'Khanwilkar, Sarika': 'sk4335@columbia.edu',\n",
       " 'Kou-Giesbrecht, Sian': 'sk4220@columbia,edu',\n",
       " 'McKenzie, Patrick': 'pfm2119@columbia.edu',\n",
       " 'Neelakantan, Amrita': 'an2601@columbia.edu',\n",
       " 'Petach, Anika': 'arp2195@columbia.edu',\n",
       " 'Quebbeman, Andrew': 'awq2101@columbia.edu',\n",
       " 'Ramesh, Vijay': 'vr2352@columbia.edu',\n",
       " 'Ribeiro Piffer, Pedro': 'prp2123@columbia.edu',\n",
       " 'Rocha Moreira, Lucas': 'lr2767@columbia.edu',\n",
       " 'Schmiege, Stephanie': 'scs2204@columbia.edu',\n",
       " 'Shah, Shailee': 'sss2254@columbia.edu',\n",
       " 'Siller, Stefanie': 'ss4812@columbia,edu',\n",
       " 'Takahashi, Maressa': 'mqt2101@columbia.edu',\n",
       " 'Taylor, Benton': 'bnt2111 @columbia.edu',\n",
       " 'Tinsman, Jennifer (Crick)': 'jc4055@columbia.edu',\n",
       " 'VanAcker, Meredith': 'mv2640@columbia.edu',\n",
       " 'Wagner, Natalie': 'nkw2113@columbia.edu'}"
      ]
     },
     "execution_count": 103,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "students"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
